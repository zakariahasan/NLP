{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:56:49.933834Z",
     "start_time": "2020-11-08T10:56:46.055451Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from tqdm import tqdm \n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy import displacy\n",
    "import string\n",
    "from spacy.gold import GoldParse\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:56:54.868045Z",
     "start_time": "2020-11-08T10:56:51.145395Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Convert json file to spaCy format.\n",
    "import plac\n",
    "import logging\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "@plac.annotations(input_file=(\"Input file\", \"option\", \"i\", str), output_file=(\"Output file\", \"option\", \"o\", str))\n",
    "\n",
    "def main(input_file=None, output_file=None):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines=[]\n",
    "        with open(input_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content']\n",
    "            entities = []\n",
    "            for annotation in data['annotation']:\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "\n",
    "                for label in labels:\n",
    "                    entities.append((point['start'], point['end'] + 1 ,label))\n",
    "\n",
    "\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "\n",
    "        print(training_data)\n",
    "\n",
    "        with open(output_file, 'wb') as fp:\n",
    "            pickle.dump(training_data, fp)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + input_file + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "if __name__ == '__main__':\n",
    "    plac.call(main)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Convert .tsv file to dataturks json format. \n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "def tsv_to_json_format(input_path,output_path,unknown_label):\n",
    "    try:\n",
    "        f=open(input_path,'r') # input file\n",
    "        fp=open(output_path, 'w') # output file\n",
    "        data_dict={}\n",
    "        annotations =[]\n",
    "        label_dict={}\n",
    "        s=''\n",
    "        start=0\n",
    "        for line in f:\n",
    "            if line[0:len(line)-1]!='.\\tO':\n",
    "                word,entity=line.split('\\t')\n",
    "                s+=word+\" \"\n",
    "                entity=entity[:len(entity)-1]\n",
    "                if entity!=unknown_label:\n",
    "                    if len(entity) != 1:\n",
    "                        d={}\n",
    "                        d['text']=word\n",
    "                        d['start']=start\n",
    "                        d['end']=start+len(word)-1  \n",
    "                        try:\n",
    "                            label_dict[entity].append(d)\n",
    "                        except:\n",
    "                            label_dict[entity]=[]\n",
    "                            label_dict[entity].append(d) \n",
    "                start+=len(word)+1\n",
    "            else:\n",
    "                data_dict['content']=s\n",
    "                s=''\n",
    "                label_list=[]\n",
    "                for ents in list(label_dict.keys()):\n",
    "                    for i in range(len(label_dict[ents])):\n",
    "                        if(label_dict[ents][i]['text']!=''):\n",
    "                            l=[ents,label_dict[ents][i]]\n",
    "                            for j in range(i+1,len(label_dict[ents])): \n",
    "                                if(label_dict[ents][i]['text']==label_dict[ents][j]['text']):  \n",
    "                                    di={}\n",
    "                                    di['start']=label_dict[ents][j]['start']\n",
    "                                    di['end']=label_dict[ents][j]['end']\n",
    "                                    di['text']=label_dict[ents][i]['text']\n",
    "                                    l.append(di)\n",
    "                                    label_dict[ents][j]['text']=''\n",
    "                            label_list.append(l)                          \n",
    "                            \n",
    "                for entities in label_list:\n",
    "                    label={}\n",
    "                    label['label']=[entities[0]]\n",
    "                    label['points']=entities[1:]\n",
    "                    annotations.append(label)\n",
    "                data_dict['annotation']=annotations\n",
    "                annotations=[]\n",
    "                json.dump(data_dict, fp)\n",
    "                fp.write('\\n')\n",
    "                data_dict={}\n",
    "                start=0\n",
    "                label_dict={}\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process file\" + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "\n",
    "tsv_to_json_format(\"zak_ner.tsv\",'zak_ner.json','abc')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "entity=[]\n",
    "entity_types=[]\n",
    "for n in range(0,len(df['meta'])):\n",
    "    if type(df['meta'].loc[n])==str:\n",
    "        if 'SIZE' in list(eval(df['meta'].loc[n]).keys()):\n",
    "            size_list=eval(df['meta'].loc[n])['SIZE']\n",
    "            for m in range(0,len(size_list)):\n",
    "                size_list_deep=size_list[m].lstrip().rstrip().upper()\n",
    "                if size_list_deep not in entity:\n",
    "                    if size_list_deep not in size_exceptions:\n",
    "                        entity.append(size_list_deep)\n",
    "                        entity_types.append('SIZE')\n",
    "        if 'COLOR' in list(eval(df['meta'].loc[n]).keys()):\n",
    "            color_list=eval(df['meta'].loc[n])['COLOR']\n",
    "            for m in range(0, len(color_list)):\n",
    "                color_item=color_list[m].title()\n",
    "                if ', ' in color_item:\n",
    "                    color_item_list=color_item.split(', ')\n",
    "                    for l in range(0,len(color_item_list)):\n",
    "                        color_item_list_deep=color_item_list[l].lstrip().rstrip()\n",
    "                        if len(color_item_list_deep)!=0:\n",
    "                            if color_item_list_deep not in entity:\n",
    "                                if color_item_list_deep.isnumeric()==False:\n",
    "                                    if color_item_list_deep not in color_exceptions:\n",
    "                                        entity.append(color_item_list_deep)\n",
    "                                        entity_types.append('COLOR')\n",
    "                                            \n",
    "                else:\n",
    "                    color_item_list_deep=color_list[m].lstrip().rstrip().title()\n",
    "                    if len(color_item_list_deep)!=0:\n",
    "                        if color_item_list_deep not in entity:\n",
    "                            if color_item_list_deep.isnumeric()==False:\n",
    "                                if color_item_list_deep not in color_exceptions:\n",
    "                                    entity.append(color_item_list_deep)\n",
    "                                    entity_types.append('COLOR')   \n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "entity_2=[]\n",
    "entity_types_2=[]\n",
    "titles_unique_brand=[]\n",
    "description_unique_brand=[]\n",
    "for n in range(0,len(df['brand'])):\n",
    "    if df['brand'].loc[n] not in entity_2:\n",
    "        if df['brand'].loc[n] != 'â€¢':\n",
    "            entity_2.append(df['brand'].loc[n])\n",
    "            entity_types_2.append('BRAND')\n",
    "            titles_unique_brand.append(df['title'].loc[n])\n",
    "            if type(df['description'].loc[n])==str:\n",
    "                description_unique_brand.append(df['description'].loc[n])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "entity_3=[]\n",
    "entity_types_3=[]\n",
    "for n in range(0,len(df['meta'])):\n",
    "    if type(df['meta'].loc[n])==str:\n",
    "        if 'GENDER' in list(eval(df['meta'].loc[n]).keys()):\n",
    "            gender_list=eval(df['meta'].loc[n])['GENDER']\n",
    "            for m in range(0, len(gender_list)):\n",
    "                gender_list_deep=gender_list[m].lstrip().rstrip().title()\n",
    "                if gender_list_deep not in entity_3:\n",
    "                    if (gender_list_deep != '[Unisex]' and gender_list_deep != '[Female]' and gender_list_deep != '[Male]'):\n",
    "                        gender_list_deep_list=gender_list_deep.split(', ')\n",
    "                        for gender_list_deep_list_item in gender_list_deep_list:\n",
    "                            if gender_list_deep_list_item not in entity_3:\n",
    "                                entity_3.append(gender_list_deep_list_item)\n",
    "                                entity_types_3.append('GENDER')\n",
    "        if 'AGE_GROUP' in list(eval(df['meta'].loc[n]).keys()):\n",
    "            age_group_list=eval(df['meta'].loc[n])['AGE_GROUP']\n",
    "            for m in range(0, len(age_group_list)):\n",
    "                age_group_list_deep=age_group_list[m].lstrip().rstrip().title()\n",
    "                if age_group_list_deep not in entity_3:\n",
    "                    entity_3.append(age_group_list_deep)\n",
    "                    entity_types_3.append('AGE_GROUP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_annotation(title):\n",
    "    \n",
    "    # Initial annotation, charactors' ranges are highly overlapped\n",
    "    entity_set_list=[]\n",
    "    entity_dict={}\n",
    "    entity_set_range_list=[]\n",
    "    for m in range(0,len(entity)):\n",
    "        if entity[m] in title:\n",
    "            index_i=title.find(entity[m])\n",
    "            index_f=index_i+len(entity[m])\n",
    "            if (index_i != 0) and (index_f != len(title)):\n",
    "                if (title[index_i-1] == ' ' and title[index_f] == ' '):\n",
    "                    #print(entity[m])\n",
    "                    entity_tuple=(index_i, index_f, entity_types[m])\n",
    "                    entity_set_list.append(entity_tuple)\n",
    "                    entity_set_range_list.append(range(index_i, index_f))\n",
    "            if (index_i == 0) and (index_f != len(title)):\n",
    "                if (title[index_f] == ' '):\n",
    "                    #print(entity[m])\n",
    "                    entity_tuple=(index_i, index_f, entity_types[m])\n",
    "                    entity_set_list.append(entity_tuple)\n",
    "                    entity_set_range_list.append(range(index_i, index_f))\n",
    "            if (index_i != 0) and (index_f == len(title)):\n",
    "                if (title[index_i-1] == ' '):\n",
    "                    #print(entity[m])\n",
    "                    entity_tuple=(index_i, index_f, entity_types[m])\n",
    "                    entity_set_list.append(entity_tuple)\n",
    "                    entity_set_range_list.append(range(index_i, index_f))\n",
    "\n",
    "    # Second Step: Get rid of overlapped charactors' ranges\n",
    "    entity_set_list_2=[]\n",
    "    for n, entity_set_range_1 in enumerate(entity_set_range_list):\n",
    "        entity_set_range_test=set(entity_set_range_1)\n",
    "        inter=0\n",
    "        entity_set_list_2_temp=[]\n",
    "        for m, entity_set_range_2 in enumerate(entity_set_range_list):\n",
    "            if entity_set_range_1 != entity_set_range_2:\n",
    "                interss=entity_set_range_test.intersection(entity_set_range_2)\n",
    "                #entity_set_list_2_temp=[]\n",
    "                if interss==set():\n",
    "                    inter += 1\n",
    "                else:\n",
    "                    if set(entity_set_range_1)>set(entity_set_range_2):\n",
    "                        if entity_set_list[n] not in entity_set_list_2_temp:\n",
    "                            entity_set_list_2_temp.append(entity_set_list[n])\n",
    "                    elif set(entity_set_range_1)<set(entity_set_range_2):\n",
    "                        if entity_set_list[m] not in entity_set_list_2_temp:\n",
    "                            entity_set_list_2_temp.append(entity_set_list[m])\n",
    "        \n",
    "        if m == inter:\n",
    "            if entity_set_list[n] not in entity_set_list_2:\n",
    "                entity_set_list_2.append(entity_set_list[n])\n",
    "        else:\n",
    "            for entity_set_list_2_temp_item in entity_set_list_2_temp:\n",
    "                if entity_set_list_2_temp_item not in entity_set_list_2:\n",
    "                    entity_set_list_2.append(entity_set_list_2_temp_item)\n",
    "    \n",
    "    # Third Step: Get rid of overlapped charactors' ranges further if any\n",
    "    entity_set_list_3=[]\n",
    "    for n, item_1 in enumerate(entity_set_list_2):\n",
    "        item_1_range=range(item_1[0],item_1[1])\n",
    "        inter=0\n",
    "        entity_set_list_2_temp=[]\n",
    "        for m, item_2 in enumerate(entity_set_list_2):\n",
    "            item_2_range=range(item_2[0],item_2[1])\n",
    "            if item_1_range != item_2_range:\n",
    "                interss=set(item_1_range).intersection(item_2_range)\n",
    "                if interss == set():\n",
    "                    inter += 1\n",
    "                else:\n",
    "                    if set(item_1_range)>set(item_2_range):\n",
    "                        if item_1 not in entity_set_list_2_temp:\n",
    "                            entity_set_list_2_temp.append(item_1)\n",
    "                    elif set(item_1_range)<set(item_2_range):\n",
    "                        if item_2 not in entity_set_list_2_temp:\n",
    "                            entity_set_list_2_temp.append(item_2)\n",
    "        if m == inter:\n",
    "            entity_set_list_3.append(item_1)\n",
    "        else:\n",
    "            for entity_set_list_2_temp_item in entity_set_list_2_temp:\n",
    "                if entity_set_list_2_temp_item not in entity_set_list_3:\n",
    "                    entity_set_list_3.append(entity_set_list_2_temp_item)\n",
    "    \n",
    "    # Fourth Step: Get rid of overlapped charactors' ranges further if any\n",
    "    entity_set_list_4=[]\n",
    "    for n, item_1 in enumerate(entity_set_list_3):\n",
    "        item_1_range=range(item_1[0],item_1[1])\n",
    "        inter=0\n",
    "        entity_set_list_2_temp=[]\n",
    "        for m, item_2 in enumerate(entity_set_list_3):\n",
    "            item_2_range=range(item_2[0],item_2[1])\n",
    "            if item_1_range != item_2_range:\n",
    "                interss=set(item_1_range).intersection(item_2_range)\n",
    "                if interss == set():\n",
    "                    inter += 1\n",
    "                else:\n",
    "                    if set(item_1_range)>set(item_2_range):\n",
    "                        if item_1 not in entity_set_list_2_temp:\n",
    "                            entity_set_list_2_temp.append(item_1)\n",
    "                    elif set(item_1_range)<set(item_2_range):\n",
    "                        if item_2 not in entity_set_list_2_temp:\n",
    "                            entity_set_list_2_temp.append(item_2)\n",
    "        if m == inter:\n",
    "            entity_set_list_4.append(item_1)\n",
    "        else:\n",
    "            for entity_set_list_2_temp_item in entity_set_list_2_temp:\n",
    "                if entity_set_list_2_temp_item not in entity_set_list_4:\n",
    "                    entity_set_list_4.append(entity_set_list_2_temp_item)\n",
    "    \n",
    "    # Construct annotation\n",
    "    entity_dict['entities']=entity_set_list_4\n",
    "    annotation_n=(title, entity_dict)\n",
    "    return annotation_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_0=[]\n",
    "for n in range(0,len(titles_unique_brand)):\n",
    "    annotation=obtain_annotation(titles_unique_brand[n])\n",
    "    if annotation[1]['entities']!=[]:\n",
    "        TRAIN_DATA_0.append(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_1=[]\n",
    "for n in range(0,len(description_unique_brand)):\n",
    "    annotation=obtain_annotation(description_unique_brand[n])\n",
    "    if annotation[1]['entities']!=[]:\n",
    "        TRAIN_DATA_1.append(annotation)\n",
    "TRAIN_DATA = TRAIN_DATA_0 + TRAIN_DATA_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(\"Ner_Pickle.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(TRAIN_DATA, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:56:54.906545Z",
     "start_time": "2020-11-08T10:56:54.870554Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"Ner_Pickle.txt\", \"rb\") as fp:   \n",
    "    DATA = pickle.load(fp)\n",
    "TRAIN_DATA=DATA[0:100]\n",
    "TEST_DATA=DATA[101:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:56:55.611356Z",
     "start_time": "2020-11-08T10:56:55.602598Z"
    }
   },
   "outputs": [],
   "source": [
    "def built_model(data,iterations,drop=0.2):\n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank('xx')  \n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes): \n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Statring iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [text],  \n",
    "                    [annotations],  \n",
    "                    drop=drop,  \n",
    "                    sgd=optimizer,  \n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T21:44:14.497244Z",
     "start_time": "2020-11-02T21:44:14.494043Z"
    }
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:57:26.533771Z",
     "start_time": "2020-11-08T10:56:56.602438Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statring iteration 0\n",
      "{'ner': 454.16092656680775}\n",
      "Statring iteration 1\n",
      "{'ner': 348.53016700584993}\n",
      "Statring iteration 2\n",
      "{'ner': 335.6208784650256}\n",
      "Statring iteration 3\n",
      "{'ner': 225.72551010572485}\n",
      "Statring iteration 4\n",
      "{'ner': 298.4570892778641}\n",
      "Statring iteration 5\n",
      "{'ner': 181.81861411529565}\n",
      "Statring iteration 6\n",
      "{'ner': 213.67335806500316}\n",
      "Statring iteration 7\n",
      "{'ner': 194.15879235694246}\n",
      "Statring iteration 8\n",
      "{'ner': 148.21225148669745}\n",
      "Statring iteration 9\n",
      "{'ner': 109.74437068745043}\n",
      "CPU times: user 56.9 s, sys: 968 ms, total: 57.9 s\n",
      "Wall time: 29.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = built_model(TRAIN_DATA,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:57:41.883803Z",
     "start_time": "2020-11-08T10:57:41.629254Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(nlp, open('model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T11:05:17.768625Z",
     "start_time": "2020-11-08T11:05:17.385525Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 90.0%\n",
      "Precision : 0.91\n",
      "Recall : 0.9\n",
      "F-score : 0.89\n"
     ]
    }
   ],
   "source": [
    "tp=0\n",
    "tr=0\n",
    "tf=0\n",
    "ta=0\n",
    "c=0.\n",
    "# GENERATING THE CLASSIFICATION REPORT\n",
    "for text,annot in TEST_DATA:\n",
    "    doc_to_test=nlp(text)\n",
    "\n",
    "    d={}\n",
    "    for ent in doc_to_test.ents:\n",
    "        d[ent.label_]=[]\n",
    "    for ent in doc_to_test.ents:\n",
    "        d[ent.label_].append(ent.text)\n",
    "   \n",
    "    for ent in doc_to_test.ents:\n",
    "        d[ent.label_]=[0,0,0,0,0,0]\n",
    "    for ent in doc_to_test.ents:\n",
    "        doc_gold_text= nlp.make_doc(text)\n",
    "        gold = GoldParse(doc_gold_text, entities=annot.get(\"entities\"))\n",
    "        y_true = [ent.label_ if ent.label_ in x else 'Not '+ent.label_ for x in gold.ner]\n",
    "        y_pred = [x.ent_type_ if x.ent_type_ ==ent.label_ else 'Not '+ent.label_ for x in doc_to_test]  \n",
    "        # print(text,y_pred)\n",
    "        if(d[ent.label_][0]==0):\n",
    "            #print(\"For Entity \"+ent.label_+\"\\n\")   \n",
    "            #f.write(classification_report(y_true, y_pred)+\"\\n\")\n",
    "            (p,r,f,s)= precision_recall_fscore_support(y_true,y_pred,average='weighted')\n",
    "            a=accuracy_score(y_true,y_pred)\n",
    "            d[ent.label_][0]=1\n",
    "            d[ent.label_][1]+=p\n",
    "            d[ent.label_][2]+=r\n",
    "            d[ent.label_][3]+=f\n",
    "            d[ent.label_][4]+=a\n",
    "            d[ent.label_][5]+=1\n",
    "    c+=1\n",
    "#print(d)\n",
    "for i in d:\n",
    "    #print(\"\\n For Entity \"+i+\"\\n\")\n",
    "    print(\"Accuracy : \"+str((d[i][4]/d[i][5])*100)+\"%\")\n",
    "    print(\"Precision : \"+str(round(d[i][1]/d[i][5],2)))\n",
    "    print(\"Recall : \"+str(round(d[i][2]/d[i][5],2)))\n",
    "    print(\"F-score : \"+str(round(d[i][3]/d[i][5],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:58:43.412601Z",
     "start_time": "2020-11-08T10:58:42.895372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to /Users/zakaria/anaconda3/ML-Material/Integrify/project/sp_zak\n",
      "Loading from /Users/zakaria/anaconda3/ML-Material/Integrify/project/sp_zak\n",
      "Entities [('94 cm', 'SIZE')]\n",
      "Entities []\n",
      "Entities [('Ziwi Peak', 'BRAND')]\n",
      "Entities [('Finger', 'BRAND')]\n",
      "Entities [('4/0', 'SIZE')]\n",
      "Entities [('Golden', 'BRAND')]\n",
      "Entities []\n",
      "Entities [('Ayleen', 'BRAND')]\n",
      "Entities []\n",
      "Entities [('170 mm', 'SIZE'), ('20 mm', 'SIZE')]\n",
      "Entities []\n",
      "Entities [('Laivastonsininen', 'COLOR')]\n",
      "Entities [('Headwear', 'COLOR'), ('Sininen', 'COLOR')]\n",
      "Entities [('Valkoinen', 'COLOR'), ('ASICS SportStyle', 'BRAND')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('Ruskea', 'COLOR'), ('R.M. Williams', 'BRAND')]\n",
      "Entities [('VihreÃ¤', 'COLOR')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('21', 'SIZE')]\n",
      "Entities [('CK', 'BRAND')]\n",
      "Entities [('Nahkiss', 'BRAND')]\n",
      "Entities []\n",
      "Entities [('Givenchy', 'BRAND')]\n",
      "Entities [('VihreÃ¤', 'COLOR')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('Gummy', 'BRAND')]\n",
      "Entities []\n",
      "Entities [('FDB', 'BRAND')]\n",
      "Entities [('ClaÃ©', 'COLOR')]\n",
      "Entities [('94', 'SIZE')]\n",
      "Entities [('Philips', 'BRAND'), ('330 ml', 'SIZE')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('JÃ¤nnelooppi', 'BRAND'), ('valkoinen', 'COLOR')]\n",
      "Entities [('Burberry', 'BRAND')]\n",
      "Entities [('Nilkkurit', 'BRAND')]\n",
      "Entities [('Monkey Teething', 'BRAND')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('Vaaleanpunainen', 'COLOR')]\n",
      "Entities [('Impulse', 'COLOR')]\n",
      "Entities [('American', 'BRAND')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('85', 'SIZE')]\n",
      "Entities []\n",
      "Entities [('Nashi', 'BRAND')]\n",
      "Entities []\n",
      "Entities [('500 mg', 'SIZE')]\n",
      "Entities [('Sininen', 'COLOR')]\n",
      "Entities [('Vaaleanpunainen', 'BRAND')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('HANUKEii', 'BRAND')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('Penny', 'BRAND'), ('Punainen', 'COLOR')]\n",
      "Entities [('Green', 'BRAND')]\n",
      "Entities [('Zenz', 'BRAND')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('NestemÃ¤inen', 'BRAND'), ('15 ml', 'SIZE')]\n",
      "Entities [('65 cm', 'SIZE')]\n",
      "Entities [('Studio for', 'BRAND')]\n",
      "Entities [('10 cm', 'SIZE'), ('4', 'SIZE')]\n",
      "Entities [('LastenkengÃ¤t Naturino', 'BRAND')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('blue', 'COLOR'), ('poikien', 'GENDER')]\n",
      "Entities [('Best', 'BRAND'), ('L', 'SIZE')]\n",
      "Entities []\n",
      "Entities [('Palashampoo', 'BRAND')]\n",
      "Entities []\n",
      "Entities [('Pusakka', 'BRAND')]\n",
      "Entities [('Reflect', 'COLOR')]\n",
      "Entities [('Nils', 'BRAND')]\n",
      "Entities [('Teltonika', 'BRAND')]\n",
      "Entities [('Beige', 'COLOR')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('naisten', 'GENDER')]\n",
      "Entities [('naisten', 'GENDER')]\n",
      "Entities [('I', 'BRAND')]\n",
      "Entities []\n",
      "Entities [('Extreme', 'BRAND')]\n",
      "Entities [('Baseus', 'BRAND')]\n",
      "Entities []\n",
      "Entities [('Breitling', 'BRAND')]\n"
     ]
    }
   ],
   "source": [
    "output_dir=Path(\"/Users/zakaria/anaconda3/ML-Material/Integrify/project/sp_zak\")\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)#\n",
    "\n",
    "    # test the saved model\n",
    "    print(\"Loading from\", output_dir)\n",
    "    nlp2 = spacy.load(output_dir)\n",
    "    for text, _ in TEST_DATA:\n",
    "        doc = nlp2(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        #print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:58:50.621477Z",
     "start_time": "2020-11-08T10:58:50.599371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('94 cm', 'SIZE')]\n",
      "Entities []\n",
      "Entities [('Ziwi Peak', 'BRAND')]\n"
     ]
    }
   ],
   "source": [
    "for text, _ in TEST_DATA[0:3]:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:59:52.045176Z",
     "start_time": "2020-11-08T10:59:51.792581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">California Hyppykeppi on hauska ulkolelu, jonka avulla lapsi saa harjoiteltua tasapainoa ja motoriikkaa. Hyppykeppi on \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    94 cm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SIZE</span>\n",
       "</mark>\n",
       " ja kestÃ¤Ã¤ 20-50 kg.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Tom Ford Chloe Aurinkolasit Musta Tom Ford Sunglasses</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ziwi Peak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">BRAND</span>\n",
       "</mark>\n",
       " Lammas - kissanruoka 400g</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Lady \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Finger\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">BRAND</span>\n",
       "</mark>\n",
       " Minivibraattori Kulta</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Kemiallisesti teroitettu Black Nickel -pinnoitettu 1-h ruohosuojakoukku. Soveltuu muun muassa hauen ja kuhan kalastukseen jigillÃ¤ tai madolla ruohikkoisissa vesissÃ¤. Voidaan kÃ¤yttÃ¤Ã¤ myÃ¶skin esimerkiksi lippa- ja lusikkauistinten koukkuna. Koot 1/0, 2/0, 3/0 ja \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    4/0\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SIZE</span>\n",
       "</mark>\n",
       ". Pussissa 3kpl.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Golden\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">BRAND</span>\n",
       "</mark>\n",
       " Curl 506 Rose  Gold Curler</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">KengÃ¤t Dkode  SARINA</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">SToys \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ayleen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">BRAND</span>\n",
       "</mark>\n",
       " Sauva</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">KengÃ¤t Keys  CLOSE</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">&lt;p&gt;Ruotsalainen Light My Fire on kehittÃ¤nyt uuden FireGrill -retkihalsterin. Kompakti FireGrill kulkee kÃ¤tevÃ¤sti erÃ¤retkeilijÃ¤n, veneilijÃ¤n, karavaanarin tai mÃ¶kkeilijÃ¤n mukana. FireGrillin avulla ruoan voi lÃ¤mmittÃ¤Ã¤, kypsentÃ¤Ã¤ tai paahtaa nuotiolla tai takkatulen Ã¤Ã¤ressÃ¤ helposti, nopeasti ja siististi.&lt;/p&gt; &lt;p&gt;Ruostumattomasta terÃ¤ksestÃ¤ valmistettu FireGrill on sÃ¤Ã¤dettÃ¤vissÃ¤ grillattavan ruuan koon mukaan. Sen vÃ¤lissÃ¤ liha, kala, purilaiset tai kasvikset kypsentyvÃ¤t vaivattomasti. KÃ¤ytÃ¶n jÃ¤lkeen halsteri voidaan pakata esimerkiksi MealKit- tai LunchKit-ruokailuastiaan eikÃ¤ siten sotke reppua.&lt;/p&gt; &lt;p&gt;FireGrillissÃ¤ on helppokÃ¤yttÃ¶inen kiinnitysmekanismi, jolla sen saa nopeasti kiinni metsÃ¤stÃ¤ lÃ¶ytyvÃ¤Ã¤n keppiin. Kun halsterin taittaa auki ja lukitsee, se toimii pidempÃ¤nÃ¤ paistoritilÃ¤nÃ¤ â€“ kÃ¤tevÃ¤Ã¤ paistaessa kookkaampia saaliita. Kuten kaikki Light My Firen tuotteet, halsteri on lujatekoinen, pakkautuu kompaktisti ja on helppo puhdistaa.&lt;/p&gt; &lt;p&gt;FireGrill vie luonnossa syÃ¶misen makkaranpaistoa pidemmÃ¤lle. Halsteri tekee ruokailusta hauskaa ja helppoa. Halsterin lisÃ¤ksi tarvitset vain kepin, jonka pÃ¤Ã¤hÃ¤n FireGrill kiinnittyy. Bon appÃ©tit!&lt;/p&gt; &lt;p&gt;Vinkki:&lt;br/&gt; Kasviksien lihan grillaamisen lisÃ¤ksi hiillostettavaksi kÃ¤yvÃ¤t erityisesti rasvaiset kalat, kuten lohi, silakka, lahna tai sÃ¤ynÃ¤vÃ¤. Ennen hiillostamista kalat perataan, suolataan ja tarvittaessa suomustetaan. Halsterin ritilÃ¤ voidaan voidella ruokaÃ¶ljyllÃ¤ tai muulla ruuanlaittorasvalla tarttumisen estÃ¤miseksi. Kala on kypsÃ¤Ã¤, kun sen selkÃ¤evÃ¤ irtoaa helposti.&lt;/p&gt; &lt;p&gt;Mitat: sivu \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    170 mm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SIZE</span>\n",
       "</mark>\n",
       ", korkeus \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    20 mm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">SIZE</span>\n",
       "</mark>\n",
       ". Paino vain 106g.&lt;/p&gt;</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Goldspeed C9205 ( 165/70-10 TL 27N Compound 2103 BLUE, E4 )</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Takki Janet &amp; Joyce \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Laivastonsininen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">COLOR</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Achieve Hat-Wool Blend Accessories \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Headwear\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">COLOR</span>\n",
       "</mark>\n",
       " Beanies \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sininen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">COLOR</span>\n",
       "</mark>\n",
       " J. Lindeberg Ski</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Japan S Matalavartiset Sneakerit Tennarit \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Valkoinen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">COLOR</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ASICS SportStyle\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">BRAND</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Smartsign Fhd Player By Qbic</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Aurelia Probiotic Skincare Citrus Botanical Cream Deodorant</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Macquaire G Shoes Chelsea Boots \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ruskea\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">COLOR</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    R.M. Williams\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">BRAND</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Kid  Sippy Cap \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    VihreÃ¤\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">COLOR</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Diesel Women Ufst-Stars-Threepack Stri</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Mamalicious. KierrÃ¤tetty polyamidi tai nailon on synteettinen materiaali, joka tuotetaan kerÃ¤tyistÃ¤ ylijÃ¤Ã¤mÃ¤kankaista. KierrÃ¤tetty nailon tarjoaa samat ominaisuudet kuin tavallinen nailon ja vÃ¤hentÃ¤Ã¤ samalla tekstiilijÃ¤tteen mÃ¤Ã¤rÃ¤Ã¤ sekÃ¤ uusien materiaalien tarvetta.. KierrÃ¤tetty.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n in range(0,20):\n",
    "    doc = nlp(TEST_DATA[n][0])\n",
    "    spacy.displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
